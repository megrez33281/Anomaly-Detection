# Anomaly Detection Project - 實作日誌

這份文件記錄了 HW2: Anomaly Detection 專案的整個實作流程、決策和思考過程。



## 階段一：模型診斷與數據增強
1.  **問題分析**
    * 執行了初步的訓練流程，發現雖然訓練損失(Train Loss)能正常下降，但驗證集上的AUROC分數始終在 0.5 左右，表示模型完全沒有學到區分正常與異常樣本的能力  
    * **診斷**
        這是一個典型現象，說明模型學到了一個「捷徑」  
        由於訓練數據過於單一（固定的尺寸和視角），模型只需學會重建一個「模糊的平均影像」就能降低損失，而無需學習圖像真正的、細粒度的紋理特徵  
        因此，當面對紋紋理錯位的 `CutPaste`樣本時，其重建誤差與正常樣本沒有顯著差異  

2.  **解決方案**：
    * **數據增強**
        為了迫使模型學習更具泛化能力的紋理特徵，在訓練階段引入了數據增強  
    * **具體實作**
        修改了`main.py`中的`train_transform`，將原本固定的`Resize`替換為`transforms.RandomResizedCrop`  
        這會讓模型在每次訓練時都看到不同縮放、不同裁剪位置的圖像，從而無法再依賴簡單的低頻資訊作弊，必須學習到更本質的特徵  
    *   同時將 `Config.EPOCHS`增加到30，給模型更長的學習時間來適應這個更困難的任務  

## 階段二：初步提交與驗證有效性確認
1.  **目的**
    使用已有模型進行一次Kaggle提交，以獲得真實測試集上的基準分數，並驗證內部驗證集的有效性  
2.  **執行**
    使用僅經過`RandomResizedCrop`增強訓練的模型（尚未加入 `CoarseDropout`進行推論並將生成的 `submission.csv`提交至 Kaggle  
3.  **結果**
    Kaggle 公開排行榜分數為**0.5277**
4.  **結論**：
    *   這個分數與我們內部使用 `CutPaste` 偽異常驗證集得到的AUROC分數（約 0.5054）非常接近  

## 階段三：應對模型「走捷徑」問題

1. **問題**
    綜合階段二的診斷與階段三的結果，確認了核心問題在於模型學會了「走捷徑」 
    它只學會了足以降低訓練損失的「低品質重建」（例如重建出模糊影像）技巧，而沒有學習到對「異常檢測」這個任務有用的、關於高頻紋理的精細特徵   
2.  **策略與原理**：們引入更強的數據增強方法 `CoarseDropout`  
    * **原理**
        `CoarseDropout`會在訓練過程中，隨機地從輸入影像上「挖掉」數個矩形區塊（填上黑色） 
        為了要重建這些被挖掉的未知區域，模型不能再依賴簡單的模糊化，而必須去理解周圍的上下文和紋理，才能「腦補」出合理的內容  
        這個過程會強迫模型去學習影像的真實、高頻的細節特徵  

3.  **執行**：
    *   修改了`Dataset.py`，將數據增強流程從`torchvision`遷移至 `albumentations` 函式庫，以便使用 `CoarseDropout`  
    *   在訓練資料的轉換流程(`train_transform`) 中，在原有的`RandomResizedCrop`基礎上加入了 `CoarseDropout`  
4.  **結果**：AUROC 分數仍停留在0.5左右  



## 分析
換過多種方式、調整參數且經過仔細檢查，發現以上述方式訓練出的AutoEncoder AUROC仍停留在0.5左右  
這代表目前的模型完全沒有進行有效的學習  
我仔細檢查了資料集，發現其中的物體分為15類別：  
1. 拉鍊
2. 螺絲
3. 鐵網
4. 螺母
5. 瓶子
6. 藥片
7. 磁磚
8. 牙刷
9. 膠囊
10. 榛果
11. 電晶體
12. 電纜
13. 皮革
14. 木板
15. 地毯
另外，不同於手寫數字的異常偵測，此處的資料集每一個類別之間的差異的極為巨大  
個人認為，這會導致上述將所有圖片一股腦丟給AutoEncoder學習的方式反而讓模型無法很好學習到不同類別物體的特徵  
最後反而可能導致任何圖片丟給AutoEncoder重建後得到特徵相似的圖片  
AUCROC一值卡在0.5左右就是一個很好的證明，模型將所有輸入的圖片都重建成-特徵相似的圖片，自然無論正常圖片或是異常圖片，輸出模型嘔得到的圖片都會非常相似   
總結：上述方式訓練AutoEncoder根本沒辦法讓模型學會重建個個類別的圖片，只能讓模型學會將所有圖片重建成一個未知但相似的物體  



## 階段四：視覺化診斷與結論 
1.  **目的**
    為了直觀地驗證「模型只學會重建模糊影像」的猜測，編寫了 `visualize_output.py` 腳本來觀察模型的實際輸出  
2.  **發現**
    腳本執行後，將原始影像與重建影像進行了對比  
    結果完全印證了先前的猜測：**無論輸入是什麼類別的物體，模型都只會輸出一張模糊、無法辨識的通用影像**  
    *左：原始影像 / 右：模型重建結果*  
    ![Reconstruction Result](reconstruction_results/compare_222.png)
    ![Reconstruction Result](reconstruction_results/compare_154.png)
    ![Reconstruction Result](reconstruction_results/compare_587.png)
    ![Reconstruction Result](reconstruction_results/compare_924.png)
    ![Reconstruction Result](reconstruction_results/compare_1175.png)
3.  **結論**
    這個視覺證據證明了，讓單一模型學習5種差異巨大的物體類別的策略是失敗的  
    模型為了在所有類別上都最小化重建誤差，最終選擇了「走捷徑」，即學習重建一個所有影像的「平均特徵」，從而完全喪失了對精細紋理和特定物體結構的辨識能力  
    這也完美解釋了為何AUROC分數始終無法突破0.5的隨機猜測水平  

## 後續計畫
考慮到AutoEncoder的架構都弄好了，再換其他的不太好，此處我決定先利用分類器將圖片分類，再用不同AutoEncoder重建
1. 先嘗試單類別的AutoEncoder確認有效
2. 加入分類器，將不同的圖片分類（或者手動分類）

## 階段五：單類別模型概念驗證-牙刷 
1.  **目的**
    驗證「為每個類別訓練專屬模型」的策略是否優於先前將所有類別混合訓練的統一模型  
    選擇「牙刷」作為第一個實驗對象，因為它的樣本數最少，最具挑戰性。 

2.  **策略**
    *  只挑選牙刷的圖片作為訓練資料投入模型
    *  沿用現有的`UNetAutoencoder`架構及`main.py`中的訓練邏輯，不作變動。
3.  **結果**
    *  根據輸出紀錄，模型在第一個Epoch的`Validation AUROC`即達到 **0.6389**  
    *  然而，從第二個 Epoch 開始，AUROC 分數迅速下降，並在0.5的隨機水準附近劇烈震盪  

4.  **結論**
    *  **假設得到驗證**
        AUROC分數在初期顯著超過0.5，這證明了先前的假設——單類別模型是正確的方向  
        模型在專注於單一類別時，確實能夠學習到區分正常與異常的有效特徵  

    *  **暴露新問題：小樣本不穩定性**
        由於牙刷的訓練樣本僅有54張，模型在極少的數據上迅速過擬合  
        這導致了後續Epoch泛化能力的喪失和驗證分數的不穩定
    *  **新的提示**
        這也帶來了一個提示，後續為每個類別的物體單獨訓練模型時，或許可以為每個物體單獨設置參數、驗證方式
            
5.  **後續步驟**
    下一步，嘗試換一個樣本數更多的類別：地毯 來重複實驗，以驗證在數據充足的情況下，訓練過程是否能更加穩定並達到更高的效能  


## 階段六：真實數據驗證與「紋理破壞」策略修正
1.  **背景**
    在「地毯」類別的訓練中，我們採用了新的「紋理破壞」(`TextureDamage`) 驗證策略，並在幾個Epoch內就達到了 `1.0` 的AUROC  
    這結果過於理想，讓我懷疑驗證任務是否過於簡單，無法反映真實的檢測能力  
2.  **目的**
    為了檢驗模型的真實實力，決定使用來自`test`資料夾中、未曾見過的真實地毯圖片（包含正常與異常）來進行一次測試  
3.  **策略**：
    *   載入AUROC達到1.0的模型  
    *   對指定的3張正常地毯圖片（`1512`, `832`, `1700`）和3張異常地毯圖片（`1144`, `640`, `97`）進行推論，並計算其異常分數  
4.  **結果**：
    *   推論分數完全無法區分正常與異常樣本。
    *   分數最高的樣本 (`1700.png`, 分數 `0.050`）反而是正常圖片  
    *   正常與異常樣本的分數區間高度重疊，模型沒有展現出任何有效的鑑別能力  



## 階段七：模型崩潰診斷與架構修復
1.  **背景**
    階段七的真實數據驗證得出了一個災難性的結果  
    一個AUROC為1.0的模型，在真實世界中卻表現得像隨機猜測一樣，個人認為有可能又出現了與先前一樣的模型根本沒有學會重建物體
2.  **發現：模型崩潰 (Model Collapse)**
    *  視覺化結果
        模型根本沒有重建任何東西，無論輸入為何，它都只會輸出一張**純灰色的圖片**  
        這完美解釋了階段七的失敗——模型並非無法分辨細微瑕疵，而是根本沒有學習影像的任何特徵  
    * *左：原始影像 / 右：模型重建結果*  
        ![模型崩潰輸出](inference_results/compare_1512.png)


5.  **根本原因分析**：
    *  在審查`UNet_Autoencoder_Model.py`後，發現U-Net架構的**跳接層 (Skip Connections)**沒有加上  
    *  沒有了跳接層，所有高解析度的細節資訊都在編碼過程中遺失，解碼器無法獲得足夠的資訊來重建圖像，最終只能輸出所有像素的平均值（灰色），導致模型崩潰  
    *  此外，由於當前的驗證方式不可靠，以所謂最佳權重儲存的模型參數可能並不可靠，應該嘗試各個epoch中模型的參數，才能判斷模型有沒有學會重建

6.  **修復**：
    * 在`UNet_Autoencoder_Model.py` 中，重新連接了所有跳接層 (`torch.cat`)  
    * 同步修正了解碼器各層的輸入通道數，以匹配跳接後的新維度  
    * 新增**偵錯模式**，儲存訓練時的每個epoch的權重，逐一嘗試其真實的重建能力






