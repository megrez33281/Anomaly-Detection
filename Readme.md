# Anomaly Detection Project - 實作日誌

這份文件記錄了 HW2: Anomaly Detection 專案的整個實作流程、決策和思考過程。



## 階段一：模型診斷與數據增強
1.  **問題分析**
    * 執行了初步的訓練流程，發現雖然訓練損失(Train Loss)能正常下降，但驗證集上的AUROC分數始終在 0.5 左右，表示模型完全沒有學到區分正常與異常樣本的能力  
    * **診斷**
        這是一個典型現象，說明模型學到了一個「捷徑」  
        由於訓練數據過於單一（固定的尺寸和視角），模型只需學會重建一個「模糊的平均影像」就能降低損失，而無需學習圖像真正的、細粒度的紋理特徵  
        因此，當面對紋紋理錯位的 `CutPaste`樣本時，其重建誤差與正常樣本沒有顯著差異  

2.  **解決方案**：
    * **數據增強**
        為了迫使模型學習更具泛化能力的紋理特徵，在訓練階段引入了數據增強  
    * **具體實作**
        修改了`main.py`中的`train_transform`，將原本固定的`Resize`替換為`transforms.RandomResizedCrop`  
        這會讓模型在每次訓練時都看到不同縮放、不同裁剪位置的圖像，從而無法再依賴簡單的低頻資訊作弊，必須學習到更本質的特徵  
    *   同時將 `Config.EPOCHS`增加到30，給模型更長的學習時間來適應這個更困難的任務  

## 階段二：初步提交與驗證有效性確認
1.  **目的**
    使用已有模型進行一次Kaggle提交，以獲得真實測試集上的基準分數，並驗證內部驗證集的有效性  
2.  **執行**
    使用僅經過`RandomResizedCrop`增強訓練的模型（尚未加入 `CoarseDropout`進行推論並將生成的 `submission.csv`提交至 Kaggle  
3.  **結果**
    Kaggle 公開排行榜分數為**0.5277**
4.  **結論**：
    *   這個分數與我們內部使用 `CutPaste` 偽異常驗證集得到的AUROC分數（約 0.5054）非常接近  

## 階段三：應對模型「走捷徑」問題

1. **問題**
    綜合階段二的診斷與階段三的結果，確認了核心問題在於模型學會了「走捷徑」 
    它只學會了足以降低訓練損失的「低品質重建」（例如重建出模糊影像）技巧，而沒有學習到對「異常檢測」這個任務有用的、關於高頻紋理的精細特徵   
2.  **策略與原理**：們引入更強的數據增強方法 `CoarseDropout`  
    * **原理**
        `CoarseDropout`會在訓練過程中，隨機地從輸入影像上「挖掉」數個矩形區塊（填上黑色） 
        為了要重建這些被挖掉的未知區域，模型不能再依賴簡單的模糊化，而必須去理解周圍的上下文和紋理，才能「腦補」出合理的內容  
        這個過程會強迫模型去學習影像的真實、高頻的細節特徵  

3.  **執行**：
    *   修改了`Dataset.py`，將數據增強流程從`torchvision`遷移至 `albumentations` 函式庫，以便使用 `CoarseDropout`  
    *   在訓練資料的轉換流程(`train_transform`) 中，在原有的`RandomResizedCrop`基礎上加入了 `CoarseDropout`  
4.  **結果**：AUROC 分數仍停留在0.5左右  



## 分析
換過多種方式、調整參數且經過仔細檢查，發現以上述方式訓練出的AutoEncoder AUROC仍停留在0.5左右  
這代表目前的模型完全沒有進行有效的學習  
我仔細檢查了資料集，發現其中的物體分為15類別：  
1. 拉鍊
2. 螺絲
3. 鐵網
4. 螺母
5. 瓶子
6. 藥片
7. 磁磚
8. 牙刷
9. 膠囊
10. 榛果
11. 電晶體
12. 電纜
13. 皮革
14. 木板
15. 地毯
另外，不同於手寫數字的異常偵測，此處的資料集每一個類別之間的差異的極為巨大  
個人認為，這會導致上述將所有圖片一股腦丟給AutoEncoder學習的方式反而讓模型無法很好學習到不同類別物體的特徵  
最後反而可能導致任何圖片丟給AutoEncoder重建後得到特徵相似的圖片  
AUCROC一值卡在0.5左右就是一個很好的證明，模型將所有輸入的圖片都重建成-特徵相似的圖片，自然無論正常圖片或是異常圖片，輸出模型嘔得到的圖片都會非常相似   
總結：上述方式訓練AutoEncoder根本沒辦法讓模型學會重建個個類別的圖片，只能讓模型學會將所有圖片重建成一個未知但相似的物體  



## 階段四：視覺化診斷與結論 
1.  **目的**
    為了直觀地驗證「模型只學會重建模糊影像」的猜測，編寫了 `visualize_output.py` 腳本來觀察模型的實際輸出  
2.  **發現**
    腳本執行後，將原始影像與重建影像進行了對比  
    結果完全印證了先前的猜測：**無論輸入是什麼類別的物體，模型都只會輸出一張模糊、無法辨識的通用影像**  
    *左：原始影像 / 右：模型重建結果*  
    ![Reconstruction Result](reconstruction_results/compare_222.png)
    ![Reconstruction Result](reconstruction_results/compare_154.png)
    ![Reconstruction Result](reconstruction_results/compare_587.png)
    ![Reconstruction Result](reconstruction_results/compare_924.png)
    ![Reconstruction Result](reconstruction_results/compare_1175.png)
3.  **結論**
    這個視覺證據證明了，讓單一模型學習5種差異巨大的物體類別的策略是失敗的  
    模型為了在所有類別上都最小化重建誤差，最終選擇了「走捷徑」，即學習重建一個所有影像的「平均特徵」，從而完全喪失了對精細紋理和特定物體結構的辨識能力  
    這也完美解釋了為何AUROC分數始終無法突破0.5的隨機猜測水平  

## 後續計畫
考慮到AutoEncoder的架構都弄好了，再換其他的不太好，此處我決定先利用分類器將圖片分類，再用不同AutoEncoder重建
1. 先嘗試單類別的AutoEncoder確認有效
2. 加入分類器，將不同的圖片分類（或者手動分類）

## 階段五：單類別模型概念驗證-牙刷 
1.  **目的**
    驗證「為每個類別訓練專屬模型」的策略是否優於先前將所有類別混合訓練的統一模型  
    選擇「牙刷」作為第一個實驗對象，因為它的樣本數最少，最具挑戰性。 

2.  **策略**
    *  只挑選牙刷的圖片作為訓練資料投入模型
    *  沿用現有的`UNetAutoencoder`架構及`main.py`中的訓練邏輯，不作變動。
3.  **結果**
    *  根據輸出紀錄，模型在第一個Epoch的`Validation AUROC`即達到 **0.6389**  
    *  然而，從第二個 Epoch 開始，AUROC 分數迅速下降，並在0.5的隨機水準附近劇烈震盪  

4.  **結論**
    *  **假設得到驗證**
        AUROC分數在初期顯著超過0.5，這證明了先前的假設——單類別模型是正確的方向  
        模型在專注於單一類別時，確實能夠學習到區分正常與異常的有效特徵  

    *  **暴露新問題：小樣本不穩定性**
        由於牙刷的訓練樣本僅有54張，模型在極少的數據上迅速過擬合  
        這導致了後續Epoch泛化能力的喪失和驗證分數的不穩定
    *  **新的提示**
        這也帶來了一個提示，後續為每個類別的物體單獨訓練模型時，或許可以為每個物體單獨設置參數、驗證方式
            
5.  **後續步驟**
    下一步，嘗試換一個樣本數更多的類別：地毯 來重複實驗，以驗證在數據充足的情況下，訓練過程是否能更加穩定並達到更高的效能  


## 階段六：真實數據驗證與「紋理破壞」策略修正
1.  **背景**
    在「地毯」類別的訓練中，我們採用了新的「紋理破壞」(`TextureDamage`) 驗證策略，並在幾個Epoch內就達到了 `1.0` 的AUROC  
    這結果過於理想，讓我懷疑驗證任務是否過於簡單，無法反映真實的檢測能力  
2.  **目的**
    為了檢驗模型的真實實力，決定使用來自`test`資料夾中、未曾見過的真實地毯圖片（包含正常與異常）來進行一次測試  
3.  **策略**：
    *   載入AUROC達到1.0的模型  
    *   對指定的3張正常地毯圖片（`1512`, `832`, `1700`）和3張異常地毯圖片（`1144`, `640`, `97`）進行推論，並計算其異常分數  
4.  **結果**：
    *   推論分數完全無法區分正常與異常樣本。
    *   分數最高的樣本 (`1700.png`, 分數 `0.050`）反而是正常圖片  
    *   正常與異常樣本的分數區間高度重疊，模型沒有展現出任何有效的鑑別能力  



## 階段七：模型崩潰診斷與架構修復
1.  **背景**
    階段七的真實數據驗證得出了一個災難性的結果  
    一個AUROC為1.0的模型，在真實世界中卻表現得像隨機猜測一樣，個人認為有可能又出現了與先前一樣的模型根本沒有學會重建物體
2.  **發現：模型崩潰 (Model Collapse)**
    *  視覺化結果
        模型根本沒有重建任何東西，無論輸入為何，它都只會輸出一張**純灰色的圖片**  
        這完美解釋了階段七的失敗——模型並非無法分辨細微瑕疵，而是根本沒有學習影像的任何特徵  
    * *左：原始影像 / 右：模型重建結果*  
        ![模型崩潰輸出](inference_results/compare_1512.png)


5.  **根本原因分析**：
    *  在審查`UNet_Autoencoder_Model.py`後，發現U-Net架構的**跳接層 (Skip Connections)**沒有加上  
    *  沒有了跳接層，所有高解析度的細節資訊都在編碼過程中遺失，解碼器無法獲得足夠的資訊來重建圖像，最終只能輸出所有像素的平均值（灰色），導致模型崩潰  
    *  此外，由於當前的驗證方式不可靠，以所謂最佳權重儲存的模型參數可能並不可靠，應該嘗試各個epoch中模型的參數，才能判斷模型有沒有學會重建

6.  **修復**：
    * 在`UNet_Autoencoder_Model.py` 中，重新連接了所有跳接層 (`torch.cat`)  
    * 同步修正了解碼器各層的輸入通道數，以匹配跳接後的新維度  
    * 新增**偵錯模式**，儲存訓練時的每個epoch的權重，逐一嘗試其真實的重建能力


## 階段八：學習歷程視覺化與最佳權重定位

1.  **目的**
    在修復U-Net架構並啟用「偵錯模式」後，透過視覺化每一輪的重建結果，來深入理解模型的學習過程，並從中手動找出最佳的模型權重。
2.  **策略**：
    * 使用修復後的模型在「偵錯模式」下完整訓練30個Epochs，並儲存所有權重  
    * 遍歷所有儲存的權重，並對固定的正常與異常測試圖片生成重建對比圖  
3.  **發現與分析**：
    * **Epoch 1 (訓練初期)**
        第一輪EPOCH，模型輸出完全灰色的方塊  
        ![Epoch 1 重建](視覺化資料對比/模型能力驗證_地毯/epoch_1/compare_1512.png)
    * **Epoch 5 (出現變化)**
        到了第五輪EPOCH，重建出的圖片已經初步出現變化
        ![Epoch 5 重建](視覺化資料對比/模型能力驗證_地毯/epoch_5/compare_1512.png)
    * **Epoch 10 (出現紋理)**
        到了第10輪EPOCH，重建出的圖片已經出現紋理
        ![Epoch 10 重建](視覺化資料對比/模型能力驗證_地毯/epoch_10/compare_1512.png)
    * **Epoch 15 (學習甜蜜點)**
        此時的模型表現達到最佳  
        它能夠高度還原正常地毯的紋理細節，但對於異常圖片中的瑕疵（一個小黑點），模型無法理解因此在對應位置產生了一片模糊的重建  
        這種「對正常特徵的精通」和「對異常-特徵的無知」所造成的重建差異，正是偵測異常的核心  
        * 正常圖片重建 (Epoch 15):
            ![正常圖片重建 (Epoch 15)](視覺化資料對比/模型能力驗證_地毯/epoch_15/compare_1512.png)
        * 異常圖片重建 (Epoch 15):
            ![異常圖片重建 (Epoch 15)](視覺化資料對比/模型能力驗證_地毯/epoch_15/compare_1144.png)
    * **Epoch 30 (過擬合期)**：
        模型訓練過度  
        從重建結果看，它開始輸出純黑色的圖像，這代表模型的輸出值極度趨近於0  
        這可能是因為模型為了在有瑕疵的訓練增強數據上降低損失，學習到了一個錯誤的策略  
        此現象也與訓練日誌中，損失值在約16個epoch後不降反升的趨勢完全吻合  
        ![Epoch 30 重建](視覺化資料對比/模型能力驗證_地毯/epoch_30/compare_1512.png)

4.  **結論**：
    *   **偵錯模式的成功**：透過「儲存所有Epoch權重」的策略，繞過了不可靠的AUROC指標，從視覺上定位到了模型的最佳狀態  
    *   **最佳權重確認**：`model_地毯_epoch_15.pth`是迄今為止得到的**重建能力最強**的權重
    *   **U-Net 架構驗證**：修復後的U-Net架構現在可以有效地學習和重建複雜的紋理特徵  

## 階段九：重建損失的量化分析與結論
1.  **目的**
    在階段八中，從視覺上判斷Epoch 15是最佳權重  
    本階段的目的是從**量化**角度，驗證這個「最佳」權重的模型，其重建損失是否真的能區分真實世界中的正常與異常樣本  
2.  **策略**：
    * 遍歷1到30所有epoch的權重  
    * 對每一個epoch的模型，都計算6張地毯測試圖片的像素級重建損失（MSE+SSIM），並將結果彙總成表格  

3.  **結果與分析**：
    * 完整的epoch分數總結表驗證了視覺上的觀察：所有圖片的重建損失在**Epoch 16**左右達到最低點，之後便開始急劇上升，證明了模型的學習在16個epoch後發生了潰  
    * 然而，最關鍵的發現是，即使在表現最好的Epoch 16，正常樣本和異常樣本的分數仍然完全混雜在一起，無法區分   
        例如，一張正常圖片 (`1512.png`) 的分數 (`0.0114`) 比三張異常圖片中的兩張還要高  

4.  **最終結論**：
    *   這次全面的量化分析給出了決定性的證據：**對於地毯這樣的複雜紋理，單純依賴像素級的重建損失，是一個不可靠的異常評分指標。**  
    *   即使模型在視覺上達到了很好的重建效果，這個重建好壞的「程度」（損失值），也無法穩定地對應到圖片是否「異常」   

5.  **下一步：轉向特徵空間**
    不再比較圖片本身，而是比較原始圖片與重建圖片兩者在模型編碼器深層所產生的「特徵」的差異  

## 階段十：特徵空間比較法
1.  **目的**
    驗證特徵空間比較策略是否能有效區分真實的正常與異常樣本  
2.  **策略**：
    *   單獨取出Encoder，以返回編碼器在瓶頸層輸出的特徵圖
    *   將異常分數的計算方式改為：**原始圖片的特徵圖**與**重建圖片的特徵圖**之間的均方誤差(MSE)  
    *   使用此新方法，遍歷所有30個 epoch 的權重，計算6張測試圖片的「特徵空間損失」  

3.  **結果與分析**：
    *   在最佳的 **Epoch 15**，結果如下表所示：  
        | 圖片 | 真實標籤 | 特徵空間分數 |
        |:---|:---|:---|
        | `1512.png` | **正常 (N)** | 1.42e-11 |
        | `640.png` | **異常 (A)** | 1.50e-11 |
        | `97.png` | **異常 (A)** | 1.50e-11 |
        | `1144.png` | **異常 (A)** | 1.59e-11 |
        | `832.png` | **正常 (N)** | 1.91e-11 |
        | `1700.png` | **正常 (N)** | 1.91e-11 |
    *   從表格中可以清晰地看到，三張異常圖片的分數被緊密地聚集在一個區間內，而正常圖片的分數則分佈在該區間的兩側  

4.  **結論**：
    * 透過在更高維度的特徵層面進行比較，可以過濾掉無關的像素級噪音，捕獲更本質的差異  
    * 但是考慮到以此方式建立出的分布，正常的資料並不集中，而是分散在異常資料的兩側，且若依照距離來進行分群，`1512.png`其實更應該被分配在異常樣本  
    * 回顧先前訓練重建地毯的模型的輸出紀錄，在15-16 Epoch，模型重建能力最好的時期達到最低  
    * 我認為或許可以利用這點，訓練各類別物件的模型，並儲存個個類型在Loss最低時的權重  
    * 用訓練好的模型的Encoder為所有正常樣本建立各類別自己的**特徵庫**並儲存  
    * 要進行異常偵測時，便將要偵測的圖片抽取特徵後與特徵庫中的對應類別特徵進行比較，並選擇其中的最小差異，與其他同類別的要偵測的圖片的最小差異進行比較  
    * 若我的猜測正確，異常樣本的分數與正常樣本的分數之間應該會存在明顯的差異  
    * 補充：由於我認為手動為test資料庫的資料分類始終不太好，所以先嘗試利用建立的特徵庫為圖片分類後計算差異，若效果不好再另做打算  

## 階段十一：策略驗證與結論

1.  **背景**
    在階段十中確立了最終的策略：為每個類別建立一個專屬的「正常特徵庫」，並透過計算測試樣本與庫中最近鄰的「特徵空間距離」來作為異常分數  
    本階段旨在驗證這個策略的最終有效性

2.  **策略執行**
    *   **建立特徵庫**：
        1.  載入「地毯」類別的最佳模型 `model_地毯_epoch_15.pth`   
        2.  透過 `CLASS_FILENAME_MAPPING` 讀取所有280張正常地毯訓練圖片  
        3.  使用模型的Encoder提取所有正常圖片的特徵向量  
        4.  將280個特徵向量儲存為 `地毯_features.pt`  
    *   **評估**：
        1.  載入模型和 `地毯_features.pt`。
        2.  讀取指定的3張正常（`1512`, `832`, `1700`）和3張異常（`640`, `97`, `1144`）地毯測試圖片  
        3.  對每張測試圖片，計算其特徵向量與特徵庫中280個向量的最小均方誤差（MSE）距離，作為異常分數  

3.  **最終結果**
    執行評估腳本後，得到決定性的結果：

| Image    | True Label | Anomaly Score |
| :------- | :--------- | :------------ |
| 97.png   | Anomalous  | 1.114385e-06  |
| 640.png  | Anomalous  | 1.041352e-06  |
| 1144.png | Anomalous  | 5.360943e-07  |
| 1512.png | Normal     | 4.700974e-07  |
| 832.png  | Normal     | 3.949200e-07  |
| 1700.png | Normal     | 3.043197e-07  |

4.  **結論**
    *   **策略驗證成功**
        從上表可見，所有異常樣本的異常分數都明確高於所有正常樣本。這證明了**「單類別模型 + 特徵庫最小距離」**的策略是有效的  

## 階段十二：小樣本類別強健性驗證（牙刷）

1.  **目的**
    在「地毯」這個數據量充足的類別上成功後，必須驗證我們的策略對於小樣本類別是否同樣有效。「牙刷」是所有類別中樣本數最少的（僅 60 張），是最佳的壓力測試對象。

2.  **策略執行**
    *   **模型訓練**：
        1.  將 `Config.py` 中的 `TARGET_CLASS` 切換為「牙刷」。
        2.  修改 `main.py` 的儲存邏輯，使其在 `DEBUG_MODE=False` 時，儲存**驗證損失 (Validation Loss) 最低**的模型，而非 AUROC 最高的模型。
        3.  執行訓練。模型在 Epoch 18 達到最低驗證損失 `0.0961`，並儲存為 `best_model_牙刷.pth`。
    *   **建立特徵庫**：
        1.  修改 `create_feature_library.py` 以載入 `best_model_...` 格式的檔名。
        2.  執行腳本，為 60 張正常牙刷樣本建立特徵庫 `牙刷_features.pt`。
    *   **評估**：
        1.  修改 `evaluate_feature_library.py` 以載入 `best_model_...` 格式的檔名，並更新測試圖片列表。
        2.  使用 3 張正常 (`634`, `379`, `391`) 和 3 張異常 (`851`, `423`, `462`) 牙刷圖片進行評估。

3.  **最終結果**
    對「牙刷」類別的評估取得了完美的成功：

| Image   | True Label | Anomaly Score |
| :------ | :--------- | :------------ |
| 462.png | Anomalous  | 0.002898      |
| 423.png | Anomalous  | 0.001435      |
| 851.png | Anomalous  | 0.000724      |
| 634.png | Normal     | 0.000255      |
| 379.png | Normal     | 0.000180      |
| 391.png | Normal     | 0.000103      |

4.  **結論**
    *   **策略強健性驗證成功**：結果表明，即使在樣本量最少的類別上，我們的最終策略依然表現完美，正常與異常樣本的分數有著清晰的界線。這給了我們極大的信心將此策略推廣到所有類別。
    *   **最佳實踐確立**：確立了「以最低驗證損失儲存模型」為標準訓練流程，取代了之前視覺化篩選或依賴不穩定 AUROC 的方法。



# 階段十二：集成多類別模型
1. 先將所有圖片手動依據類別分類，然後為每個類別依照上面的架構建立模型

2. 分類
    為了保證inference時的自動化，會先嘗試為圖片分類，然後啟用對應模型  
    * 方法：採用兩階段混合分類法
        1. 初步分類
            先用「像素級最近鄰」法，將測試圖片與15個類別的代表圖進行比較，得出一個初步分類  
        2. 二次精煉
            如果初步分類結果是「紋理」類別（如地毯、皮革等），則啟動「顏色直方圖比較法」，在紋理類別之間進行一次更精確的比較，以修正分類結果  

3. 計分 
    在圖片被分類後啟動對應的模型，計算它相對於同類的異常程度
    * 方法
        對於被分到同一個類別的所有圖片（例如，所有被歸為「瓶子」的圖片），載入專門的「瓶子」模型
        和對應的「瓶子」特徵庫，計算出每一張圖片精確的異常分數（採用先前驗證可行的特徵空間的最小距離）

4. 動態分界
    為每一個類別，動態地劃定一條「正常/異常」的分界線
     * 方法：採用你設計的「尋找第一個異常跳躍點」策略
        1. 將一個類別內的所有分數從低到高排序
        2. 計算分數最低的前20個樣本之間的「平均正常跳躍幅度」
         3. 以此為基準，尋找整個分數序列中第一個「遠超正常水平」的跳躍點，並將該點的分數設為此類別的專屬閾值


## 確認分開訓練模型的方法不可行
經過詢問，發現此專案的規定，不能分開訓練模型，只能將所有data用於訓練一個模型


##　階段十三：回歸最初嘗試：
雖然最初的將所有圖片用於訓練一個AutoEncoder的方式失敗了，但考慮到也存在驗證方式不成立，導致儲存的最佳權重不是實際最佳權重的情況  
此處再次嘗試最初的設想，不過這次會儲存每一個epoch的權重，並以視覺化的方式比較重建效果
* 結果
    經過30個epoch的訓練，將每個epoch的權重都儲存，並以視覺化的方式分析重現能力  
    意外的發現，只有epoch 1的權重有不錯的重建能力，其他epoch的權重重建出來的結果都是一片黑
    * **Epoch 1 (訓練初期)**
        第一輪EPOCH，模型輸出就有了重建圖片的能力
        ![Epoch 1 重建](視覺化資料對比/模型能力驗證_通用/epoch_1/compare_1512.png)
    * **Epoch 2-30**
        之後2-30個epoch重建出的圖片都是一片黑
        ![Epoch 2 重建](視覺化資料對比/模型能力驗證_通用/epoch_2/compare_1512.png)
        ![Epoch 30 重建](視覺化資料對比/模型能力驗證_通用/epoch_30/compare_1512.png)

    * **Epoch 1 (其他物件)**
        為了確保模型真的傭有重建能力，嘗試了其他物件，依樣能夠獲得有結構的圖片
        ![Epoch 1 重建](視覺化資料對比/模型能力驗證_通用/epoch_1/compare_998.png)
* 結論
    雖然用epoch 1的權重就能達到看起來不錯的重建效果，但是因為這只是epoch 1，沒辦法保證模型是真的學到了相關特徵，還是只是把圖片輸入後原樣（或只改變一點）出  
    不過考慮到就視覺效果而言，已經達到不錯的重建效果，我還是決定用與先前嘗試相似的方式，將模型作為特徵提取器，提取特徵後用相似度計算異常分數  

    假設模型確實學習到了重建圖片的能力，那麼在模型遇見異常圖片的情況下，還能夠將其重建的與輸入圖片差異不大，或許便是模型過於強大，加上skip connection將輸入圖片的細節直接跳接，讓模型可以輕鬆重建細節  
    這可能也是一個優化方向：**藉由調整U-Net架構（降低跳接次數、不在高層跳接、縮小模型等）嘗試讓模型學會只重建學習過的物件的樣子（也就是異常圖片會因為沒見過的特徵而被重建成語原本圖片差很多的樣子）**  
    不過考慮到當前可能是模型能力過強的情況，換句話說就是模型抽取特徵能力很強，我認為可以先嘗試原本的方案（利用模型抽取特徵後分析）  


## 階段十四：單一模型+ 特徵分群
1.  **背景**
    在確認 Epoch 1 的模型是唯一有重建能力的權重後，推進了原定的「單一模型 + 特徵分群」策略  

2.  **策略**
    *   **假設**: Epoch 1模型的Encoder已學到足以在特徵空間中區分15個類別的資訊 淤
    *   **執行**:
        1.  使用Epoch 1模型的Encoder對全部正常訓練圖片提取特徵，建立統一特徵庫`feature_library_epoch_1.pt`  
        2.  對這個特徵庫執行K-Means演算法(K=15)，試圖自動找出15個物體類別的群組  

3.  **結果與分析**
    *   K-Means 產生的15個群組，每一個都是一團混亂  
    *   **沒有任何一個群組由單一類別主導**。每個群組的「純度」（即群組內佔比最高的類別所佔的比例）都低於 1%  
    *   這明確表示，由Epoch 1模型產生的特徵向量，對於不同類別的物體來說，是完全不可區分的。所有特徵在潛在空間中擠成一團，形成了一個無法分辨的「特徵奇異點」。
4.  **結論**
    *   **假設被推翻**：Epoch 1 的 Encoder 並**沒有**學到足以區分不同類別的特徵。它學會的只是一種對所有輸入都適用的、非常通用的「模糊化」或「平均化」的特徵提取方式  
    *   **策略的終結**：這證明了「單一模型 + 特徵分群」的策略，在當前的設定下是**行不通的**  



